
simple crawler using scrapy framework . scrapy have useful features like : avoiding duplicate urls,limitation on depth , defining request rate and ... .
you need to install scrapy :
`pip install scrapy`
and run the crawler :
`scrapy crawl hamshahri -o ham.json`

corpus.txt  will be maked in this directory after running.
settings can be found in :'crawler/settings.py' and 'crawler/spiders/hamshahri_spider.py'


